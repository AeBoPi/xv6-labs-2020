整个 lab3 的目的：探索页表，修改页表以简化从用户态拷贝数据到内核态的方法。
# 实验2 A kernel pate table per process
>
xv6 原本的设计是，用户进程在用户态使用各自的用户态页表，但是一旦进入内核态（例如使用了系统调用），则切换到内核页表（通过修改 satp 寄存器，trampoline.S）。然而这个内核页表是全局共享的，也就是全部进程进入内核态都共用同一个内核态页表：
>
```
// vm.c
pagetable_t kernel_pagetable; // 全局变量，共享的内核页表
```
## 实验目标
本 Lab 目标是让每一个进程进入内核态后，都能有自己的独立内核页表，为第三个实验做准备

### 创建进程内核页表与内核栈
1. 首先在进程的结构体 proc 中，添加一个kernelpgtbl，用于存储进程专享的内核态页表。

2. 修改kvminit (vm.c)
>
    内核需要依赖内核页表内一些固定的映射的存在才能正常工作，例如 UART 控制、硬盘界面、中断控制等。而 kvminit 原本只为全局内核页表 kernel_pagetable 添加这些映射。我们抽象出来一个可以为任何我们自己创建的内核页表添加这些映射的函数 kvm_map_pagetable()。

    kvminit_newpgtbl()函数用来新建内核页表

    原有的 kvminit() 函数中仍然保留新建的全局内核页表，用于 boot 过程，以及无进程运行时使用

    为 kvmmap() 函数和 kvmpa() 函数添加一个pagetable_t类型参数
>
3. 处理内核栈
>
    原本的 xv6 设计中，所有处于内核态的进程都共享同一个页表，即意味着共享同一个地址空间。由于 xv6 支持多核/多进程调度，同一时间可能会有多个进程处于内核态，所以需要对所有处于内核态的进程创建其独立的内核态内的栈，也就是内核栈，供给其内核态代码执行过程。

    xv6 在启动过程中，会在 procinit() 中为所有可能的 64 个进程位都预分配好内核栈 kstack，具体为在高地址空间里，每个进程使用一个页作为 kstack，并且两个不同 kstack 中间隔着一个无映射的 guard page 用于检测栈溢出错误。

    在 xv6 原来的设计中，内核页表本来是只有一个的，所有进程共用，所以需要为不同进程创建多个内核栈，并 map 到不同位置（见 procinit() 和 KSTACK 宏）。而我们的新设计中，每一个进程都会有自己独立的内核页表，并且每个进程也只需要访问自己的内核栈，而不需要能够访问所有 64 个进程的内核栈。所以可以将所有进程的内核栈 map 到其各自内核页表内的固定位置（不同页表内的同一逻辑地址，指向不同物理内存）。
    
>
### 切换到进程内核页表
// kernel/proc.c 在调度器将 CPU 交给进程执行之前，切换到该进程对应的内核页表

这样，每个进程执行的时候，就都会在内核态采用自己独立的内核页表了
### 释放进程内核页表
// kernel/vm.c 添加释放内核页表的函数kvm_free_kernelpgtbl()，递归释放整个多级页表数，由free walk()修改而来。

// kernel/proc.c 添加释放进程的内核栈部分

注意到我们的修改影响了其他代码： virtio 磁盘驱动 virtio_disk.c 中调用了 kvmpa() 用于将虚拟地址转换为物理地址，这一操作在我们修改后的版本中，需要传入进程的内核页表。


# Lab3 Simplify copyin/copyinstr (hard)
## 实验目标
>
在进程的内核态页表中维护一个用户态页表映射的副本，这样使得内核态也可以对用户态传进来的指针（逻辑地址）进行解引用。这样做相比原来 copyin 的实现的优势是，原来的 copyin 是通过软件模拟访问页表的过程获取物理地址的，而在内核页表内维护映射副本的话，可以利用 CPU 的硬件寻址功能进行寻址，效率更高并且可以受快表加速。

要实现这样的效果，我们需要在每一处内核对用户页表进行修改的时候，将同样的修改也同步应用在进程的内核页表上，使得两个页表的程序段（0 到 PLIC 段）地址空间的映射同步。
>
### 准备
1. 首先实现一些工具方法，多数是参考现有方法改造得来
~~~
// kernel/vm.c

// 注：需要在 defs.h 中添加相应的函数声明，这里省略。

// 将 src 页表的一部分页映射关系拷贝到 dst 页表中。
// 只拷贝页表项，不拷贝实际的物理页内存。
// 成功返回0，失败返回 -1
int
kvmcopymappings(pagetable_t src, pagetable_t dst, uint64 start, uint64 sz)
{}


// 与 uvmdealloc 功能类似，将程序内存从 oldsz 缩减到 newsz。但区别在于不释放实际内存
// 用于内核页表内程序内存映射与用户页表程序内存映射之间的同步
uint64
kvmdealloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz)
{}
~~~
2. 接下来，为映射程序内存做准备。实验中提示内核启动后，能够用于映射程序内存的地址范围是[0,PLIC)，我们将把进程程序内存映射到其内核页表的这个范围内，首先要确保这个范围没有和其他映射冲突。
>
查阅 xv6 book 可以看到，在 PLIC 之前还有一个 CLINT（核心本地中断器）的映射，该映射会与我们要 map 的程序内存冲突。查阅 xv6 book 的 Chapter 5 以及 start.c 可以知道 CLINT 仅在内核启动的时候需要使用到，而用户进程在内核态中的操作并不需要使用到该映射。
![alt text](image.png)

所以**修改 kvm_map_pagetable()**，去除 CLINT 的映射，这样进程内核页表就不会有 CLINT 与程序内存映射冲突的问题。但是由于全局内核页表也使用了 kvm_map_pagetable() 进行初始化，并且内核启动的时候需要 CLINT 映射存在，**故在 kvminit() 中，另外单独给全局内核页表映射 CLINT**。

同时在 exec 中加入检查，防止程序内存超过 PLIC。
>
### 同步映射
后面的步骤就是在每个修改到进程用户页表的位置，都将相应的修改同步到进程内核页表中。一共要修改：fork()、exec()、growproc()、userinit()。

### 替换 copyin， copyinstr 实现 vm.c